##########################################################################################################

##  I create company database with mysql with 2 tables employee department  ##
	mysql - u 'user_name' -p
	create database company;
	use company;
	create table employee(id int not null primary key auto_increment, name varchar(50) not null, sex varchar(50) not null, department_id int not null);
	create table department(id int not null primary key auto_increment, name varchar(50) not null);
	insert into employee  values (1, 'Mohamed', 'M',1211);
	insert into employee  values (2, 'ahmed', 'M',2112);
	insert into employee  values (3, 'Asmaa', 'F',1211);
	insert into department values (1211, 'IT');
	insert into department values (2112, 'R&D');
	mysqldump -u root -p company > company.sql

##   To import it i use   ##
	create database company; # empty database with the same name
	mysql -u root -p company < company.sql

##   create remote user to access database   ##
	create user 'zanaty'@'192.168.1.7' identified by '12345';
	grant all privileges on *.* to 'zanaty'@'192.168.1.7' with grant option;
	show grant for 'zanaty'@'192.168.1.7';

	# To check remote user
		mysql -u zanaty -h 192.168.1.7 -p

############################################################################################################

##  to explore database  ##
	mysql -u zanaty -h 192.168.1.7 -p
	show databases;
	show tables;
	describe employee;
	select * from employee;
	select * from employee where id = 1;
	select * from employee where id > 1;
	select * from employee department where department_id = id;

#####################################################################################################################

# start all servcies
	cd /usr/local/hadoop/
	start-all.sh

# list dataBase via sqoop
	cd /usr/local/sqoop/bin
	./sqoop-list-databases --connect jdbc:mysql://192.168.1.7:3306 --username zanaty --password 12345

# list tables via sqoop
	./sqoop-list-tables --connect jdbc:mysql://192.168.1.7:3306/Company --username zanaty --password 12345

# import to hive
	./sqoop-import --connect jdbc:mysql://192.168.1.7:3306/Company --username zanaty --password 12345 --table-name employee --hive import

	sqoop-import-all-tables --connect jdbc:mysql://192.168.1.7/myCompany --username zanaty --password 12345 --hive-import

# import to hbase
      start-hbase.sh
      sqoop import --hbase-create-table --hbase-table emp --column-family info --hbase-row-key id --connect jdbc:mysql://192.168.1.7/myCompany --username zanaty --password 12345 --table employee -- --schema myComapny --split-by id

######################################################################################################################

# show tables
	hive
	show tables;

# hive queries
	select * from employee;
	select name from employee where id = 2;

# hbase queries
	hbase not working correctly
	
#####################################################################################################################


echo "foo foo quux labs foo bar quux quux" | ./mapper.py | ./reducer.py

echo "foo foo quux labs foo bar quux quux" | ./mapper.py | sort -k1,1 | ./reducer.py

#####################################################################################################################


bin/hadoop dfs -mkdir -p /input

hadoop fs -ls /input

hadoop fs -put /home/msuser01/WordCount/input2.txt /input 

hadoop fs -ls /input

hadoop jar hadoop-streaming-2.7.1.jar -file /home/msuser01/WordCount/mapper.py -mapper /home/msuser01/WordCount/mapper.py \ 
-file /home/msuser01/WordCount/reducer.py -reducer /home/msuser01/WordCount/reducer.py -input /input/input2.txt -output /out2

hadoop fs -cat /out2/part-00000

